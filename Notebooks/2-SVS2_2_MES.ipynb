{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d225fcba-f5fb-44ec-848a-cfeadcf252d9",
   "metadata": {},
   "source": [
    "# Part 2: Take Land Surface Model multi-layered snow profiles and reduce them to Microwave Equivalent Snowpacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509e30b-8f0a-4439-b245-5696dec4038c",
   "metadata": {},
   "source": [
    "Benoit Montpetit, CPS/CRD/ECCC, 2024  \n",
    "Julien Meloche, CPS/CRD/ECCC, 2024  \n",
    "Mike Brady, CPS/CRD/ECCC, 2024  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47d8ab-1156-4725-85e3-010d816628b5",
   "metadata": {},
   "source": [
    "This notebook takes multi-layered snowpacks from land surface models and aggregates the layers to a minimum number of layers relevant to microwave radiative transfer without compromising the geophysical properties of the snowpack.  \n",
    "The methodology was developed by [Meloche et al. (2025)](https://doi.org/10.5194/tc-19-2949-2025).  \n",
    "The input data originally comes from the Soil Vegetation Snow version 2 Land Surface Model [(SVS-2; Vionnet et al,, In Prep)](NoLink) improved for Arctic snowpacks by [Woolley et al (2024)](https://doi.org/10.5194/tc-18-5685-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de6911-0b73-4f24-b069-c35a81161cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions defined in res_functions.py\n",
    "from res_functions import compute_ke, avg_snow_sum_thick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311dfc4-6848-45bd-b3b2-d58e2ed55878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf95092",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee8bd9-89f5-4752-9caa-83f313880882",
   "metadata": {},
   "source": [
    "# Process all Arctic Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea4648-371a-4fbd-a934-cc04f96cebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant Arctic SVS-2 data to TVC Exp 2018/19.\n",
    "svs_arctic = xr.open_dataset(DATA_DIR / 'SVS-2_ArcticEnsembles_TVC02.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69326fd3-69da-46ce-bbc1-1a86a967c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "svs_arctic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff6bd9-1a5c-4e11-b8d0-1c1393af8eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "layers = []\n",
    "ensembles = []\n",
    "\n",
    "for i in range(svs_arctic.sizes['ensemble']):\n",
    "    svs_arctic_temp = svs_arctic.isel(ensemble=i)[['SNODEN_ML','SNOMA_ML','SNODP','SNODOPT_ML','TSNOW_ML','SNOTYPE_ML']].to_dataframe().dropna()\n",
    "    if not svs_arctic_temp.empty:\n",
    "        for cur_date in svs_arctic_temp.index.get_level_values(0).unique():\n",
    "            for j in range(2):\n",
    "                ensembles.append(i)\n",
    "                times.append(cur_date)\n",
    "                layers.append(j)\n",
    "\n",
    "indexes = pd.MultiIndex.from_arrays([ensembles,times,layers], names=('ensemble', 'time', 'snow_layer'))\n",
    "svs_arctic_merged = pd.DataFrame({}, index=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42c0ef-7e51-444f-86c5-31381c66a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "svs_arctic_temp=svs_arctic.isel(ensemble=i)[['SNODEN_ML','SNOMA_ML','SNODP','SNODOPT_ML','TSNOW_ML','SNOTYPE_ML']].to_dataframe().dropna()\n",
    "svs_arctic_temp['ssa'] = svs_arctic_temp['SNODOPT_ML'].apply(lambda x : 6./(x*917) if x > 0 else np.nan)\n",
    "svs_arctic_temp = svs_arctic_temp.dropna()\n",
    "svs_arctic_temp['thickness'] = svs_arctic_temp[['SNODEN_ML','SNOMA_ML']].apply(lambda x : x.iloc[1] / x.iloc[0], axis = 1)\n",
    "for cur_date in svs_arctic_temp.index.get_level_values(0).unique():\n",
    "    svs_arctic_temp.loc[cur_date,'height'] = np.cumsum(svs_arctic_temp.loc[cur_date].thickness.values[::-1])[::-1]\n",
    "\n",
    "    X = pd.DataFrame({ 'ke' : compute_ke(svs_arctic_temp.loc[cur_date,['thickness', 'TSNOW_ML', 'SNODEN_ML', 'ssa', 'height']]), \n",
    "                       'height' : svs_arctic_temp.loc[cur_date].height})\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
    "    svs_arctic_temp.loc[cur_date,'label'] = kmeans.labels_\n",
    "\n",
    "svs_arctic_merged = svs_arctic_merged.join(svs_arctic_temp.loc[cur_date].groupby('label', \n",
    "                                                                                 sort = False).apply(lambda x: avg_snow_sum_thick(x, method = 'thick'),\n",
    "                                                                                                     include_groups=False).rename_axis('snow_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367b0e1-ac90-4a0a-b0c0-3f69d03299df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, svs_arctic.sizes['ensemble']):\n",
    "\n",
    "    svs_arctic_temp=svs_arctic.isel(ensemble=i)[['SNODEN_ML','SNOMA_ML','SNODP','SNODOPT_ML','TSNOW_ML','SNOTYPE_ML']].to_dataframe().dropna()\n",
    "    if not svs_arctic_temp.empty:\n",
    "        svs_arctic_temp['ssa'] = svs_arctic_temp['SNODOPT_ML'].apply(lambda x : 6./(x*917) if x > 0 else np.nan)\n",
    "        svs_arctic_temp = svs_arctic_temp.dropna()\n",
    "        svs_arctic_temp['thickness'] = svs_arctic_temp[['SNODEN_ML','SNOMA_ML']].apply(lambda x : x.iloc[1] / x.iloc[0], axis = 1)\n",
    "    \n",
    "    \n",
    "        for cur_date in svs_arctic_temp.index.get_level_values(0).unique():\n",
    "            svs_arctic_temp.loc[cur_date,'height'] = np.cumsum(svs_arctic_temp.loc[cur_date].thickness.values[::-1])[::-1]\n",
    "\n",
    "            X = pd.DataFrame({ 'ke' : compute_ke(svs_arctic_temp.loc[cur_date,['thickness', 'TSNOW_ML', 'SNODEN_ML', 'ssa', 'height']]), \n",
    "                               'height' : svs_arctic_temp.loc[cur_date].height})\n",
    "            kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
    "            svs_arctic_temp.loc[cur_date,'label'] = kmeans.labels_\n",
    "\n",
    "            svs_arctic_merged.loc[i,cur_date] = svs_arctic_temp.loc[cur_date].groupby('label', \n",
    "                                                                               sort = False).apply(lambda x: avg_snow_sum_thick(x, method = 'thick'), \n",
    "                                                                                                   include_groups=False).rename_axis('snow_layer').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f9c6a-3472-486f-8e30-c529a7aee5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "svs_arctic_res = xr.Dataset.from_dataframe(svs_arctic_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f76c9d-f1e3-43a4-8b42-c290ff3793e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "svs_arctic_res.to_netcdf(DATA_DIR / 'SVS-2_ArcticEnsembles_TVC02_MES.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eeeefc-9ee3-4158-b5db-5cd195a31539",
   "metadata": {},
   "source": [
    "# Process the defaults outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c216f9-62bd-4a6c-b9bd-7f5c7f136315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant Default SVS-2 data to TVC Exp 2018/19.\n",
    "svs_default = xr.open_dataset(DATA_DIR / 'SVS-2_DefaultEnsembles_TVC02.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca642772-e430-4ec8-8a88-5aba6fdba6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "layers = []\n",
    "ensembles = []\n",
    "\n",
    "for i in range(svs_default.sizes['ensemble']):\n",
    "    svs_default_temp = svs_default.isel(ensemble=i)[['SNODEN_ML','SNOMA_ML','SNODP','SNODOPT_ML','TSNOW_ML','SNOTYPE_ML']].to_dataframe().dropna()\n",
    "    if not svs_default_temp.empty:\n",
    "        for cur_date in svs_default_temp.index.get_level_values(0).unique():\n",
    "            for j in range(2):\n",
    "                ensembles.append(i)\n",
    "                times.append(cur_date)\n",
    "                layers.append(j)\n",
    "\n",
    "indexes = pd.MultiIndex.from_arrays([ensembles,times,layers], names=('ensemble', 'time', 'snow_layer'))\n",
    "svs_default_merged = pd.DataFrame({}, index=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c81052-a16c-46d8-8f65-5fb6dfdab6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "svs_default_temp=svs_default.isel(ensemble=i)[['SNODEN_ML','SNOMA_ML','SNODP','SNODOPT_ML','TSNOW_ML','SNOTYPE_ML']].to_dataframe().dropna()\n",
    "svs_default_temp['ssa'] = svs_default_temp['SNODOPT_ML'].apply(lambda x : 6./(x*917) if x > 0 else np.nan)\n",
    "svs_default_temp=svs_default_temp.dropna()\n",
    "svs_default_temp['thickness'] = svs_default_temp[['SNODEN_ML','SNOMA_ML']].apply(lambda x : x.iloc[1] / x.iloc[0], axis = 1)\n",
    "for cur_date in svs_default_temp.index.get_level_values(0).unique():\n",
    "    svs_default_temp.loc[cur_date,'height'] = np.cumsum(svs_default_temp.loc[cur_date].thickness.values[::-1])[::-1]\n",
    "\n",
    "    X = pd.DataFrame({ 'ke' : compute_ke(svs_default_temp.loc[cur_date,['thickness', 'TSNOW_ML', 'SNODEN_ML', 'ssa', 'height']]), \n",
    "                       'height' : svs_default_temp.loc[cur_date].height})\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
    "    svs_default_temp.loc[cur_date,'label'] = kmeans.labels_\n",
    "\n",
    "svs_default_merged = svs_default_merged.join(svs_default_temp.loc[cur_date].groupby('label', \n",
    "                                                                                 sort = False).apply(lambda x: avg_snow_sum_thick(x, method = 'thick'),\n",
    "                                                                                                     include_groups=False).rename_axis('snow_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf274ed9-62e9-448b-8f19-8200bd5ce349",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, svs_default.sizes['ensemble']):\n",
    "\n",
    "    svs_default_temp=svs_default.isel(ensemble=i)[['SNODEN_ML','SNOMA_ML','SNODP','SNODOPT_ML','TSNOW_ML','SNOTYPE_ML']].to_dataframe().dropna()\n",
    "    if not svs_default_temp.empty:\n",
    "        svs_default_temp['ssa'] = svs_default_temp['SNODOPT_ML'].apply(lambda x : 6./(x*917) if x > 0 else np.nan)\n",
    "        svs_default_temp=svs_default_temp.dropna()\n",
    "        svs_default_temp['thickness'] = svs_default_temp[['SNODEN_ML','SNOMA_ML']].apply(lambda x : x.iloc[1] / x.iloc[0], axis = 1)\n",
    "    \n",
    "    \n",
    "        for cur_date in svs_default_temp.index.get_level_values(0).unique():\n",
    "            svs_default_temp.loc[cur_date,'height'] = np.cumsum(svs_default_temp.loc[cur_date].thickness.values[::-1])[::-1]\n",
    "\n",
    "            X = pd.DataFrame({ 'ke' : compute_ke(svs_default_temp.loc[cur_date,['thickness', 'TSNOW_ML', 'SNODEN_ML', 'ssa', 'height']]), \n",
    "                               'height' : svs_default_temp.loc[cur_date].height})\n",
    "            kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
    "            svs_default_temp.loc[cur_date,'label'] = kmeans.labels_\n",
    "\n",
    "            svs_default_merged.loc[i,cur_date] = svs_default_temp.loc[cur_date].groupby('label', \n",
    "                                                                               sort = False).apply(lambda x: avg_snow_sum_thick(x, method = 'thick'), \n",
    "                                                                                                   include_groups=False).rename_axis('snow_layer').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33f9a7-f573-4ebf-95f3-584fbeb5f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svs_default_res = xr.Dataset.from_dataframe(svs_default_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd9ba5-f179-43ae-ab66-0d1d9e004c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "svs_default_res.to_netcdf(DATA_DIR / 'SVS-2_DefaultEnsembles_TVC02_MES.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de5fd3-a0ab-4e78-9015-b4c26d30b12b",
   "metadata": {},
   "source": [
    "# Process top 30 Arctic Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec0e692-c266-49be-98ea-b1011dd56ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant Top 30 Arctic SVS-2 data to TVC Exp 2018/19.\n",
    "svs_arctic = xr.open_dataset(DATA_DIR / 'SVS-2_ArcticTop30Ensembles_TVC02.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c86f4-013a-4d34-b7d7-f0419af19a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "layers = []\n",
    "ensembles = []\n",
    "\n",
    "for i in range(svs_arctic.sizes['ensemble']):\n",
    "    svs_arctic_temp = svs_arctic.isel(ensemble=i)[['SNODEN_ML','SNOMA_ML','SNODP','SNODOPT_ML','TSNOW_ML','SNOTYPE_ML']].to_dataframe().dropna()\n",
    "    if not svs_arctic_temp.empty:\n",
    "        for cur_date in svs_arctic_temp.index.get_level_values(0).unique():\n",
    "            for j in range(2):\n",
    "                ensembles.append(i)\n",
    "                times.append(cur_date)\n",
    "                layers.append(j)\n",
    "\n",
    "indexes = pd.MultiIndex.from_arrays([ensembles,times,layers], names=('ensemble', 'time', 'snow_layer'))\n",
    "svs_arctic_merged = pd.DataFrame({}, index=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30294118-b1bb-4c36-9691-e89151674d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "svs_arctic_temp=svs_arctic.isel(ensemble=i)[['SNODEN_ML','SNOMA_ML','SNODP','SNODOPT_ML','TSNOW_ML','SNOTYPE_ML']].to_dataframe().dropna()\n",
    "svs_arctic_temp['ssa'] = svs_arctic_temp['SNODOPT_ML'].apply(lambda x : 6./(x*917) if x > 0 else np.nan)\n",
    "svs_arctic_temp=svs_arctic_temp.dropna()\n",
    "svs_arctic_temp['thickness'] = svs_arctic_temp[['SNODEN_ML','SNOMA_ML']].apply(lambda x : x.iloc[1] / x.iloc[0], axis = 1)\n",
    "for cur_date in svs_arctic_temp.index.get_level_values(0).unique():\n",
    "    svs_arctic_temp.loc[cur_date,'height'] = np.cumsum(svs_arctic_temp.loc[cur_date].thickness.values[::-1])[::-1]\n",
    "\n",
    "    X = pd.DataFrame({ 'ke' : compute_ke(svs_arctic_temp.loc[cur_date,['thickness', 'TSNOW_ML', 'SNODEN_ML', 'ssa', 'height']]), \n",
    "                       'height' : svs_arctic_temp.loc[cur_date].height})\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
    "    svs_arctic_temp.loc[cur_date,'label'] = kmeans.labels_\n",
    "\n",
    "svs_arctic_merged = svs_arctic_merged.join(svs_arctic_temp.loc[cur_date].groupby('label', \n",
    "                                                                                 sort = False).apply(lambda x: avg_snow_sum_thick(x, method = 'thick'),\n",
    "                                                                                                     include_groups=False).rename_axis('snow_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b356b-6665-4d09-a611-a00ae929baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, svs_arctic.sizes['ensemble']):\n",
    "\n",
    "    svs_arctic_temp=svs_arctic.isel(ensemble=i)[['SNODEN_ML','SNOMA_ML','SNODP','SNODOPT_ML','TSNOW_ML','SNOTYPE_ML']].to_dataframe().dropna()\n",
    "    if not svs_arctic_temp.empty:\n",
    "        svs_arctic_temp['ssa'] = svs_arctic_temp['SNODOPT_ML'].apply(lambda x : 6./(x*917) if x > 0 else np.nan)\n",
    "        svs_arctic_temp=svs_arctic_temp.dropna()\n",
    "        svs_arctic_temp['thickness'] = svs_arctic_temp[['SNODEN_ML','SNOMA_ML']].apply(lambda x : x.iloc[1] / x.iloc[0], axis = 1)\n",
    "    \n",
    "    \n",
    "        for cur_date in svs_arctic_temp.index.get_level_values(0).unique():\n",
    "            svs_arctic_temp.loc[cur_date,'height'] = np.cumsum(svs_arctic_temp.loc[cur_date].thickness.values[::-1])[::-1]\n",
    "\n",
    "            X = pd.DataFrame({ 'ke' : compute_ke(svs_arctic_temp.loc[cur_date,['thickness', 'TSNOW_ML', 'SNODEN_ML', 'ssa', 'height']]), \n",
    "                               'height' : svs_arctic_temp.loc[cur_date].height})\n",
    "            kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
    "            svs_arctic_temp.loc[cur_date,'label'] = kmeans.labels_\n",
    "\n",
    "            svs_arctic_merged.loc[i,cur_date] = svs_arctic_temp.loc[cur_date].groupby('label', \n",
    "                                                                               sort = False).apply(lambda x: avg_snow_sum_thick(x, method = 'thick'), \n",
    "                                                                                                   include_groups=False).rename_axis('snow_layer').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c6e1f-7bab-44f6-91c1-39cc52ae9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "svs_arctic_res = xr.Dataset.from_dataframe(svs_arctic_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec240fa6-531d-4628-a309-3df75a4f4e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "svs_arctic_res.to_netcdf(DATA_DIR / 'SVS-2_ArcticTop30Ensembles_TVC02_MES.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3afaa-4b81-4c94-8d87-c10596fb8d4d",
   "metadata": {},
   "source": [
    "# Process top 30 default ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998792a7-43a0-46e6-bae5-f73f67c75c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant Top 30 Default SVS-2 data to TVC Exp 2018/19.\n",
    "svs_default = xr.open_dataset(DATA_DIR / 'SVS-2_DefaultTop30Ensembles_TVC02.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9dd97-9d8b-4a0b-96f1-0a6a87f13ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "layers = []\n",
    "ensembles = []\n",
    "\n",
    "for i in range(svs_default.sizes['ensemble']):\n",
    "    svs_default_temp = svs_default.isel(ensemble=i)[['SNODEN_ML','SNOMA_ML','SNODP','SNODOPT_ML','TSNOW_ML','SNOTYPE_ML']].to_dataframe().dropna()\n",
    "    if not svs_default_temp.empty:\n",
    "        for cur_date in svs_default_temp.index.get_level_values(0).unique():\n",
    "            for j in range(2):\n",
    "                ensembles.append(i)\n",
    "                times.append(cur_date)\n",
    "                layers.append(j)\n",
    "\n",
    "indexes = pd.MultiIndex.from_arrays([ensembles,times,layers], names=('ensemble', 'time', 'snow_layer'))\n",
    "svs_default_merged = pd.DataFrame({}, index=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a15067-2d39-4ef0-b13a-119e1c562420",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "svs_default_temp=svs_default.isel(ensemble=i)[['SNODEN_ML','SNOMA_ML','SNODP','SNODOPT_ML','TSNOW_ML','SNOTYPE_ML']].to_dataframe().dropna()\n",
    "svs_default_temp['ssa'] = svs_default_temp['SNODOPT_ML'].apply(lambda x : 6./(x*917) if x > 0 else np.nan)\n",
    "svs_default_temp=svs_default_temp.dropna()\n",
    "svs_default_temp['thickness'] = svs_default_temp[['SNODEN_ML','SNOMA_ML']].apply(lambda x : x.iloc[1] / x.iloc[0], axis = 1)\n",
    "for cur_date in svs_default_temp.index.get_level_values(0).unique():\n",
    "    svs_default_temp.loc[cur_date,'height'] = np.cumsum(svs_default_temp.loc[cur_date].thickness.values[::-1])[::-1]\n",
    "\n",
    "    X = pd.DataFrame({ 'ke' : compute_ke(svs_default_temp.loc[cur_date,['thickness', 'TSNOW_ML', 'SNODEN_ML', 'ssa', 'height']]), \n",
    "                       'height' : svs_default_temp.loc[cur_date].height})\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
    "    svs_default_temp.loc[cur_date,'label'] = kmeans.labels_\n",
    "\n",
    "svs_default_merged = svs_default_merged.join(svs_default_temp.loc[cur_date].groupby('label', \n",
    "                                                                                 sort = False).apply(lambda x: avg_snow_sum_thick(x, method = 'thick'),\n",
    "                                                                                                     include_groups=False).rename_axis('snow_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d3549-1625-40f1-ba9c-f412a2824578",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, svs_default.sizes['ensemble']):\n",
    "\n",
    "    svs_default_temp=svs_default.isel(ensemble=i)[['SNODEN_ML','SNOMA_ML','SNODP','SNODOPT_ML','TSNOW_ML','SNOTYPE_ML']].to_dataframe().dropna()\n",
    "    if not svs_default_temp.empty:\n",
    "        svs_default_temp['ssa'] = svs_default_temp['SNODOPT_ML'].apply(lambda x : 6./(x*917) if x > 0 else np.nan)\n",
    "        svs_default_temp=svs_default_temp.dropna()\n",
    "        svs_default_temp['thickness'] = svs_default_temp[['SNODEN_ML','SNOMA_ML']].apply(lambda x : x.iloc[1] / x.iloc[0], axis = 1)\n",
    "    \n",
    "    \n",
    "        for cur_date in svs_default_temp.index.get_level_values(0).unique():\n",
    "            svs_default_temp.loc[cur_date,'height'] = np.cumsum(svs_default_temp.loc[cur_date].thickness.values[::-1])[::-1]\n",
    "\n",
    "            X = pd.DataFrame({ 'ke' : compute_ke(svs_default_temp.loc[cur_date,['thickness', 'TSNOW_ML', 'SNODEN_ML', 'ssa', 'height']]), \n",
    "                               'height' : svs_default_temp.loc[cur_date].height})\n",
    "            kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
    "            svs_default_temp.loc[cur_date,'label'] = kmeans.labels_\n",
    "\n",
    "            svs_default_merged.loc[i,cur_date] = svs_default_temp.loc[cur_date].groupby('label', \n",
    "                                                                               sort = False).apply(lambda x: avg_snow_sum_thick(x, method = 'thick'), \n",
    "                                                                                                   include_groups=False).rename_axis('snow_layer').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865bba3-1f0b-467b-bfea-825a6b10f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svs_default_res = xr.Dataset.from_dataframe(svs_default_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2ac02-b48c-41b3-8d44-5860febb863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svs_default_res.to_netcdf(DATA_DIR / 'SVS-2_DefaultTop30Ensembles_TVC02_MES.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvc1819",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
